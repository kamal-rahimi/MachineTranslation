{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kamal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kamal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kamal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kamal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kamal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file provides some helper functions required to read and prepare data\n",
    "for the model\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "SPLIT_PATTERN_WITH_DILIMITER = r'([`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?\\n\\s])\\s*'\n",
    "SPLIT_PATTERN_NO_DILIMITER   = r'[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?\\n\\s]\\s*'\n",
    "\n",
    "\n",
    "def read_data(data_path):\n",
    "    \"\"\"\n",
    "    Reads data from an excel file\n",
    "    \"\"\"\n",
    "    data_set = pd.read_excel(data_path)\n",
    "    qids_raw       = data_set[\"QID\"].values\n",
    "    conditions_raw = data_set[\"CONDITION\"].values\n",
    "    outputs_raw    = data_set[\"OUTPUT\"].values\n",
    "    return qids_raw, conditions_raw, outputs_raw\n",
    "\n",
    "def write_data(qids, conditions, outputs, data_path):\n",
    "    \"\"\"\n",
    "    Writes data to excel file\n",
    "    \"\"\"\n",
    "    data_set = pd.DataFrame(list(zip(qids, conditions, outputs)),\n",
    "                            columns=[\"QID\", \"CONDITION\", \"OUTPUT\"])\n",
    "    data_set.to_excel(data_path)\n",
    "\n",
    "\n",
    "def prepare_data(qids_raw, conditions_raw, outputs_raw):\n",
    "    \"\"\"\n",
    "    Prepares data for the model by\n",
    "    Args:\n",
    "        qids_raw: Pyhon list of raw qid texts\n",
    "        conditions_raw: Pyhon list of raw condition texts\n",
    "        outputs_raw: Pyhon list of raw output texts\n",
    "    Returns:\n",
    "        qids: Pyhon list of preprocessed qid sequnces\n",
    "        conditions: Pyhon list of preprocessed condition sequnces\n",
    "        outputs: Pyhon list of preprocessed output sequnces\n",
    "        dictionaries_standardization: Pyhton list of dictionaries used for standardizing samples\n",
    "    \"\"\"\n",
    "\n",
    "    qids = []\n",
    "    conditions = []\n",
    "    outputs = []\n",
    "    dictionaries_lemanization = []\n",
    "    for qid_raw, condition_raw, output_raw in zip(qids_raw, conditions_raw, outputs_raw):\n",
    "        qid, condition, output, dictionary = preprocess_sample(qid_raw, condition_raw, output_raw)\n",
    "        qids.append(qid)\n",
    "        conditions.append(condition)\n",
    "        outputs.append(output)\n",
    "        dictionaries_lemanization.append(dictionary)\n",
    "\n",
    "    return qids, conditions, outputs, dictionaries_standardization\n",
    "\n",
    "def preprocess_sample(qid_raw, condition_raw, output_raw):\n",
    "    qid, condition, output = split_to_words(qid_raw, condition_raw, output_raw)\n",
    "    \n",
    "    qid, condition, output, dictionary_standardization = standardize_words(qid, condition, output)\n",
    "    return qid, condition, output, dictionary_standardization\n",
    "\n",
    "def split_to_words(qid_raw, condition_raw, output_raw):\n",
    "    qid       = re.split(SPLIT_PATTERN_NO_DILIMITER, str(qid_raw))\n",
    "    condition = re.split(SPLIT_PATTERN_NO_DILIMITER, str(condition_raw))\n",
    "    condition = [cond for cond in condition if cond != \" \" and cond != \"\"]\n",
    "    output    = re.split(SPLIT_PATTERN_WITH_DILIMITER, str(output_raw))\n",
    "    \n",
    "    qid       = [x.lower() for x in qid]\n",
    "    condition = [x.lower() for x in condition]\n",
    "    output    = [x.lower() for x in output]\n",
    "    \n",
    "    return qid, condition, output\n",
    "\n",
    "def standardize_words(qid, condition, output):\n",
    "    dictionary_standardization = {}\n",
    "    for index, id in enumerate(qid):\n",
    "        standard_qid = '<QID{}>'.format(index)\n",
    "        dictionary_standardization[standard_qid] = qid[index]\n",
    "        qid[index] = standard_qid\n",
    "    \n",
    "        for word_index in range(len(condition)):\n",
    "            if condition[word_index] == id:\n",
    "                condition[word_index] = standard_qid\n",
    "\n",
    "        for word_index in range(len(output)):\n",
    "            if output[word_index] == id:\n",
    "                output[word_index] = standard_qid\n",
    "\n",
    "    digit_num = 0\n",
    "    for word in condition:\n",
    "        if word.isdigit():\n",
    "            standard_digit = '<DGT{}>'.format(digit_num)\n",
    "            digit_num += 1\n",
    "            dictionary_standardization[standard_digit] = word\n",
    "\n",
    "            for word_index in range(len(condition)):\n",
    "                if condition[word_index] == word:\n",
    "                    condition[word_index] = standard_digit\n",
    "\n",
    "            for word_index in range(len(output)):\n",
    "                if output[word_index] == word:\n",
    "                    output[word_index] = standard_digit\n",
    "\n",
    "    for word in output:\n",
    "        if word.isdigit():\n",
    "            standard_digit = '<DGT{}>'.format(digit_num)\n",
    "            digit_num += 1\n",
    "            dictionary_standardization[standard_digit] = word\n",
    "            for word_index in range(len(output)):\n",
    "                if output[word_index] == word:\n",
    "                    output[word_index] = standard_digit\n",
    "    \n",
    "    condition   = ['<BOS>']  + condition + ['<EOS>']\n",
    "    output      = ['<BOS>']  + output  + ['<EOS>']\n",
    "\n",
    "    return qid, condition, output, dictionary_standardization\n",
    "\n",
    "\n",
    "def create_vocabulary(word_list, max_vocab_size):\n",
    "    \"\"\" Create Vocabulary dictionary\n",
    "    Args:\n",
    "        text(str): inout word list\n",
    "        max_vocab_size: maximum number of words in the vocabulary\n",
    "    Returns:\n",
    "        word2id(dict): word to id mapping\n",
    "        id2word(dict): id to word mapping\n",
    "    \"\"\"\n",
    "    words = [word for sample in word_list for word in sample]\n",
    "    freq = Counter(words)\n",
    "    word2id = {'<PAD>' : 0}\n",
    "    id2word = {0 : '<PAD>'}\n",
    "\n",
    "    for word, _ in freq.most_common():\n",
    "        id = len(word2id)\n",
    "        if word not in word2id:\n",
    "            word2id[word] = id\n",
    "            id2word[id] = word\n",
    "            print(word)\n",
    "        if id == max_vocab_size - 1 :\n",
    "            break\n",
    "\n",
    "    return word2id, id2word\n",
    "\n",
    "\n",
    "def replace_using_dict(list, dictionary, drop_unknown=False):\n",
    "    translated_list = []\n",
    "    for line in list:\n",
    "        if drop_unknown:\n",
    "            translated_line = [dictionary[word] for word in line if word in dictionary]\n",
    "        else:\n",
    "            translated_line = [dictionary[word] if word in dictionary else word for word in line]\n",
    "        translated_list.append(translated_line)\n",
    "    \n",
    "    return translated_list\n",
    "\n",
    "def pad_with_zero(list, max_length, pad_type):\n",
    "    padded_list = pad_sequences(list, maxlen=max_length, padding=pad_type, truncating='post')\n",
    "    return padded_list\n",
    "\n",
    "\n",
    "def log_to_shell(index, qid_raw, condition_raw, output_raw, decoded_seqeunce):\n",
    "    \"\"\" Prints information to shell\n",
    "    \"\"\"\n",
    "    print(\"Sample index\",       index)\n",
    "    print(\"QID: \",              qid_raw)\n",
    "    print(\"CONDITION: \",        condition_raw)\n",
    "    print(\"OUTPUT: \",           output_raw,'\\n')\n",
    "    print(\"Predicted OUTPUT: \", decoded_seqeunce, '\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "def plot_word_cloud(word_list):\n",
    "    words = ' '.join(word_list)\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white',\n",
    "                    stopwords = None,\n",
    "                    collocations = False,\n",
    "                    regexp=None,\n",
    "                    min_word_length=0,\n",
    "                    min_font_size = 10).generate(words) \n",
    "                         \n",
    "    #plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_length_distribution(list, max_length=100, cdf=False):\n",
    "    length_count = [0 for _ in range(max_length)]\n",
    "    #item =[list]\n",
    "    for item in list:\n",
    "        item_length = len(item)\n",
    "        if item_length < max_length:\n",
    "            length_count[item_length] += 1\n",
    "\n",
    "    length_count = np.array(length_count)\n",
    "    length_freq = length_count/np.sum(length_count)\n",
    "    if cdf:\n",
    "        length_freq = np.cumsum(length_freq)\n",
    "    plt.plot(length_freq)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    print(QIDs_process[:2],\"\\n\")\n",
    "    print(CONDITIONs_process[:2],\"\\n\")\n",
    "    print(CONDITIONs[:2],\"\\n\")\n",
    "    print(OUTPUTs_process[:2])\n",
    "    \n",
    "    all_word = [word for i in range(len(CONDITIONs_process)) for word in CONDITIONs_process[i]]\n",
    "    plot_word_cloud(all_word)\n",
    "    all_word = [word for i in range(len(OUTPUTs_process)) for word in OUTPUTs_process[i]]\n",
    "    plot_word_cloud(all_word)\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded train data set from [./data/MT_training_corpus.xlsx]\n",
      "\n",
      "<BOS>\n",
      "<EOS>\n",
      "if\n",
      "<DGT0>\n",
      "<QID1>\n",
      "ask\n",
      "<DGT1>\n",
      "<QID0>\n",
      "or\n",
      "in\n",
      "selected\n",
      "at\n",
      "only\n",
      "to\n",
      "routing\n",
      "not\n",
      "<DGT2>\n",
      "<QID2>\n",
      "respondent\n",
      "show\n",
      "a1\n",
      "question\n",
      "and\n",
      "screenout\n",
      "is\n",
      "coded\n",
      "of\n",
      "those\n",
      "a2\n",
      "instructions\n",
      "continue\n",
      "title\n",
      "terminate\n",
      "the\n",
      "screen\n",
      "out\n",
      "brand\n",
      "for\n",
      "us\n",
      "respondents\n",
      "<DGT3>\n",
      "otherwise\n",
      "then\n",
      "canada\n",
      "age\n",
      "term\n",
      "a\n",
      "a3\n",
      "end\n",
      "–\n",
      "below\n",
      "a4\n",
      "aware\n",
      "who\n",
      "skip\n",
      "above\n",
      "select\n",
      "years\n",
      "least\n",
      "answered\n",
      "than\n",
      "brands\n",
      "any\n",
      "close\n",
      "salad\n",
      "punch\n",
      "that\n",
      "<QID3>\n",
      "was\n",
      "ordered\n",
      "else\n",
      "uk\n",
      "product\n",
      "ne\n",
      "q1002–a1\n",
      "q1003–a2\n",
      "<DGT4>\n",
      "old\n",
      "qid\n",
      "exclude\n",
      "a5\n",
      "code\n",
      "yes\n",
      "all\n",
      "race\n",
      "as\n",
      "programmer\n",
      "do\n",
      "hcp\n",
      "q1002–a2\n",
      "with\n",
      "must\n",
      "isn’t\n",
      "younger\n",
      "answer\n",
      "hispanic\n",
      "does\n",
      "pn\n",
      "one\n",
      "answers\n",
      "client\n",
      "page\n",
      "did\n",
      "hidden\n",
      "new\n",
      "asked\n",
      "drop\n",
      "list\n",
      "this\n",
      "these\n",
      "label\n",
      "thank\n",
      "down\n",
      "priority\n",
      "ca\n",
      "screener\n",
      "had\n",
      "q1003–a1\n",
      "france\n",
      "months\n",
      "codes\n",
      "table\n",
      "use\n",
      "sample\n",
      "on\n",
      "used\n",
      "from\n",
      "advertising\n",
      "q42061\n",
      "price\n",
      "equal\n",
      "test\n",
      "women\n",
      "online\n",
      "china\n",
      "year\n",
      "prescribed\n",
      "info\n",
      "state\n",
      "node\n",
      "recommended\n",
      "ranch\n",
      "have\n",
      "latino\n",
      "satisfaction\n",
      "first\n",
      "based\n",
      "greater\n",
      "no\n",
      "(\n",
      ")\n",
      "\n",
      ".\n",
      "<BOS>\n",
      "<EOS>\n",
      "<DGT0>\n",
      "any\n",
      "<QID1>\n",
      "<DGT1>\n",
      ",\n",
      "<QID0>\n",
      " \n",
      "<DGT2>\n",
      "notany\n",
      ":\n",
      "between\n",
      "<QID2>\n",
      "or\n",
      "<\n",
      ">\n",
      "&\n",
      "<DGT3>\n",
      "answered\n",
      "<DGT4>\n",
      "notbetween\n",
      "q1003\n",
      "<QID3>\n",
      "all\n",
      "<DGT5>\n",
      "notanswered\n",
      "-\n",
      "none\n",
      "<DGT6>\n",
      "=\n",
      "_\n",
      "qd\n",
      "q12\n",
      "not\n",
      "q9712\n",
      "q9\n",
      "2302b\n",
      "<DGT7>\n",
      "andqe2\n",
      "q99005\n",
      "qg1\n",
      "orq9712\n",
      "/\n",
      "<DGT8>\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None, 150)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 50)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 40), (None,  30560       encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 40), ( 14560       decoder_input[0][0]              \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 50)     2050        decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 47,170\n",
      "Trainable params: 47,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      " - 5s - loss: 1.8140 - acc: 0.6131 - val_loss: 1.0649 - val_acc: 0.8028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 40) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      " - 3s - loss: 0.8291 - acc: 0.8203 - val_loss: 0.6315 - val_acc: 0.8708\n",
      "Epoch 3/30\n",
      " - 3s - loss: 0.5282 - acc: 0.8772 - val_loss: 0.6167 - val_acc: 0.8342\n",
      "Epoch 4/30\n",
      " - 2s - loss: 0.3279 - acc: 0.9248 - val_loss: 0.5624 - val_acc: 0.8633\n",
      "Epoch 5/30\n",
      " - 3s - loss: 0.2477 - acc: 0.9391 - val_loss: 0.3600 - val_acc: 0.9103\n",
      "Epoch 6/30\n",
      " - 2s - loss: 0.2134 - acc: 0.9479 - val_loss: 0.3846 - val_acc: 0.9069\n",
      "Epoch 7/30\n",
      " - 2s - loss: 0.1887 - acc: 0.9574 - val_loss: 0.3148 - val_acc: 0.9231\n",
      "Epoch 8/30\n",
      " - 2s - loss: 0.1212 - acc: 0.9727 - val_loss: 0.2900 - val_acc: 0.9164\n",
      "Epoch 9/30\n",
      " - 2s - loss: 0.1852 - acc: 0.9595 - val_loss: 0.2776 - val_acc: 0.9428\n",
      "Epoch 10/30\n",
      " - 3s - loss: 0.1335 - acc: 0.9715 - val_loss: 0.1501 - val_acc: 0.9717\n",
      "Epoch 11/30\n",
      " - 3s - loss: 0.1251 - acc: 0.9725 - val_loss: 0.2493 - val_acc: 0.9319\n",
      "Epoch 12/30\n",
      " - 3s - loss: 0.0988 - acc: 0.9778 - val_loss: 0.3006 - val_acc: 0.9236\n",
      "Epoch 13/30\n",
      " - 2s - loss: 0.1274 - acc: 0.9728 - val_loss: 0.2258 - val_acc: 0.9411\n",
      "Epoch 14/30\n",
      " - 2s - loss: 0.0810 - acc: 0.9819 - val_loss: 0.2788 - val_acc: 0.9364\n",
      "Epoch 15/30\n",
      " - 2s - loss: 0.0546 - acc: 0.9876 - val_loss: 0.2950 - val_acc: 0.9331\n",
      "Epoch 16/30\n",
      " - 3s - loss: 0.0619 - acc: 0.9860 - val_loss: 0.2219 - val_acc: 0.9492\n",
      "Epoch 17/30\n",
      " - 3s - loss: 0.0430 - acc: 0.9917 - val_loss: 0.2617 - val_acc: 0.9389\n",
      "Epoch 18/30\n",
      " - 3s - loss: 0.0733 - acc: 0.9819 - val_loss: 0.2621 - val_acc: 0.9517\n",
      "Epoch 19/30\n",
      " - 3s - loss: 0.0713 - acc: 0.9856 - val_loss: 0.2120 - val_acc: 0.9375\n",
      "Epoch 20/30\n",
      " - 3s - loss: 0.0561 - acc: 0.9880 - val_loss: 0.0818 - val_acc: 0.9747\n",
      "Epoch 21/30\n",
      " - 3s - loss: 0.0468 - acc: 0.9899 - val_loss: 0.0907 - val_acc: 0.9711\n",
      "Epoch 22/30\n",
      " - 3s - loss: 0.0488 - acc: 0.9873 - val_loss: 0.2066 - val_acc: 0.9347\n",
      "Epoch 23/30\n",
      " - 3s - loss: 0.0439 - acc: 0.9916 - val_loss: 0.1860 - val_acc: 0.9553\n",
      "Epoch 24/30\n",
      " - 3s - loss: 0.0447 - acc: 0.9902 - val_loss: 0.2124 - val_acc: 0.9583\n",
      "Epoch 25/30\n",
      " - 3s - loss: 0.0228 - acc: 0.9956 - val_loss: 0.1573 - val_acc: 0.9683\n",
      "Epoch 26/30\n",
      " - 2s - loss: 0.0277 - acc: 0.9942 - val_loss: 0.1662 - val_acc: 0.9669\n",
      "Epoch 27/30\n",
      " - 2s - loss: 0.0180 - acc: 0.9966 - val_loss: 0.2499 - val_acc: 0.9600\n",
      "Epoch 28/30\n",
      " - 3s - loss: 0.0162 - acc: 0.9976 - val_loss: 0.1107 - val_acc: 0.9836\n",
      "Epoch 29/30\n",
      " - 3s - loss: 0.0186 - acc: 0.9960 - val_loss: 0.2703 - val_acc: 0.9456\n",
      "Epoch 30/30\n",
      " - 3s - loss: 0.0172 - acc: 0.9970 - val_loss: 0.1426 - val_acc: 0.9753\n",
      "\n",
      "Trained seq2seq model saved in [./model/mt_seq2seq_model.h5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "This file reads and preproces the train dataset and creates (and trains) a seq2seq model\n",
    "using Recurrent Neurak Networks to predict a target sequnce from an input sequnce.\n",
    "openpyxl\n",
    "xlrd\n",
    "numpy\n",
    "tensorflow\n",
    "pandas\n",
    "sklearn\n",
    "\"\"\"\n",
    "\n",
    "### Import required packages\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from keras.models import Sequential, load_model, Model, Input\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation, Dropout, TimeDistributed, Bidirectional, Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "## Import helper functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tools import read_data, prepare_data, create_vocabulary, replace_using_dict, pad_with_zero\n",
    "\n",
    "## Define default training data path\n",
    "MT_TRAINING_CORPUS_PATH  = \"./data/MT_training_corpus.xlsx\"\n",
    "\n",
    "## Specify path to save model and metadata\n",
    "MT_SEQ2SEQ_MODEL_PATH    = \"./model/mt_seq2seq_model.h5\"\n",
    "MT_MODEL_CHECKPOINT_PATH = \"./model/model.chpt\"\n",
    "MT_META_DATA_FILE_PATH   = \"./model/metadata.pickle\"\n",
    "\n",
    "## Define model parameter\n",
    "# Encoder and Decoder maximum vocabulary size\n",
    "encoder_vocab_size = 150\n",
    "decoder_vocab_size = 50\n",
    "\n",
    "# Encoder and Decoder sequnces length\n",
    "encoder_seq_length = 20\n",
    "decoder_seq_length = 15\n",
    "\n",
    "# Number of training epcohs\n",
    "num_epochs = 30\n",
    "\n",
    "# Training Batch size\n",
    "batch_size = 20\n",
    "\n",
    "# Number of LSTM latend dimention in both Encoder and Decoder\n",
    "num_latent_dim = 40\n",
    "\n",
    "# Fraction of data used for validation during training the model\n",
    "validation_size = 0.1\n",
    "\n",
    "\n",
    "def data_generator(X, y, batch_size):\n",
    "    \"\"\" Creates a data genrator to feed encoder and decoder input sequnces and decoder\n",
    "    target sequnce\n",
    "    Args:\n",
    "        X: input sequnces\n",
    "        y: target sequnces\n",
    "    Returns:\n",
    "         yields a batch of encoder and decoder input sequnces and decoder target sequnce\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for j in range(random.randint(1,len(X)-batch_size)):\n",
    "            encoder_input_sequnce  = np.zeros((batch_size, encoder_seq_length, encoder_vocab_size), dtype='float32')\n",
    "            decoder_input_sequnce  = np.zeros((batch_size, decoder_seq_length, decoder_vocab_size), dtype='float32')\n",
    "            decoder_target_sequnce = np.zeros((batch_size, decoder_seq_length, decoder_vocab_size), dtype='float32')\n",
    "\n",
    "            for i, (input_seq, target_seq) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_seq):\n",
    "                    encoder_input_sequnce[i, t, word] = 1  # encoder input seq\n",
    "                for t, word in enumerate(target_seq):\n",
    "                    if t < decoder_seq_length:\n",
    "                        decoder_input_sequnce[i, t, word] = 1 # decoder input seq\n",
    "                    if t>0:\n",
    "                        decoder_target_sequnce[i, t-1, word] = 1 # decoder target seq\n",
    "\n",
    "            yield([encoder_input_sequnce, decoder_input_sequnce], decoder_target_sequnce)            \n",
    " \n",
    "\n",
    "\n",
    "def create_seq2seq_model(encoder_vocab_size, decoder_vocab_size, latent_dim):\n",
    "    \"\"\" Creates a seq2seq model using Recurrent Neural Networks(RNN).\n",
    "    The encoder consists of a left-to-right LSTM layer and outputs states to decoder.\n",
    "    The decoder is also consists of a left-to-right LSTM layer and outputs a sequence that\n",
    "    are fed to time distributed fully connected layers with softmax activation to predict \n",
    "    target sequence. \n",
    "    Args:\n",
    "        encoder_vocab_size: number of encoder tokens (i.e., encoder vocab size)\n",
    "        decoder_vocab_size: size of  decoder tokens (i.e., decoder vocab size)\n",
    "        latent_dim: number of LSTM hidden dimenetions\n",
    "    Returns:\n",
    "        model: seq2seq model\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Encoder\n",
    "    ## Input layer\n",
    "    encoder_inputs = Input(shape=(None, encoder_vocab_size), name='encoder_input')\n",
    "    ## LSTM layer\n",
    "    encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We keep encoder states and discard encoder ouput.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    ### Decoder\n",
    "    ## Input layer\n",
    "    decoder_inputs = Input(shape=(None, decoder_vocab_size), name='decoder_input')\n",
    "    ## Left to right LSTM layer\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                        initial_state=encoder_states)\n",
    "    ## Fully connected layer\n",
    "    decoder_dense = Dense(decoder_vocab_size, activation='softmax', name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    ### Model to jointly train Encoder and Decoder \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_seq2seq_inference_model(model, latent_dim):\n",
    "    \"\"\" Creates a seq2seq inference model by extracting Encoder and Decoder models\n",
    "     from the input seq2seq model.\n",
    "    Args:\n",
    "        model: a seq2seq model\n",
    "        laten_dim: number of latent dimention of the seq2seq model\n",
    "    Returns:\n",
    "        encoder_model: encoder model of input seq2seq model\n",
    "        decoder_model: decoder model of input seq2seq model\n",
    "    \"\"\"\n",
    "    ### Inference Model\n",
    "    # 1. Encode the input sequence using Encoder and return state for decoder input\n",
    "    # 2. Run one step of decoder with this intial state and \"start of sequnce\" token\n",
    "    #  as input. The output will be used as the next decoder input sequnce token\n",
    "    # 3. This procedure is repteated to predict all output sequnce \n",
    "    \n",
    "    ### Encoder Model\n",
    "    encoder_inputs = model.input[0] \n",
    "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer('encoder_lstm').output   # lstm_1\n",
    "    encoder_states = [state_h_enc, state_c_enc]\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    ### Decoder Model\n",
    "    ## Decoder State Input\n",
    "    decoder_inputs = model.input[1]\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    ## Decoder LSTM layer\n",
    "    decoder_lstm = model.get_layer('decoder_lstm')\n",
    "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h_dec, state_c_dec]\n",
    "    ## Decoder Fully connected layer\n",
    "    decoder_dense = model.get_layer('decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def train_seq2seq_model(model, X_train, X_valid, y_train, y_valid, epochs):\n",
    "    \"\"\" Compiles and trains the seq2seq model. The train data is fed to model\n",
    "    using a generator function\n",
    "    Args:\n",
    "        model: seq2seq model\n",
    "        X_train: train data input sequnce (conditions)\n",
    "        X_valid: train data input sequnce (conditions)\n",
    "        y_train: validation target sequnce sequnce (ouputs)\n",
    "        y_valid: validation target sequnce (ouputs)\n",
    "        epochs: number of epochs to train model\n",
    "    Returns:\n",
    "        model: trained seq2seq model\n",
    "    \"\"\"\n",
    "\n",
    "    # Model is trainined to minimize cross enthrop between true target sequnce\n",
    "    # and predicted target sequnce\n",
    "    # Optimizer is set to Nadam and accuracy is used as metric\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Nadam',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    # Creats data genrators to feed train and validation data\n",
    "    train_data_generator = data_generator(X_train, y_train, batch_size)\n",
    "    valid_data_generator = data_generator(X_valid, y_valid, batch_size)\n",
    "    \n",
    "    # Define callback fo model checkpoint\n",
    "    callbacks = [ModelCheckpoint(MT_MODEL_CHECKPOINT_PATH, save_best_only=True, save_weights_only=False)]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit_generator(train_data_generator,\n",
    "                        validation_data=valid_data_generator,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=2,\n",
    "                        steps_per_epoch=len(X_train)/batch_size,\n",
    "                        validation_steps=len(X_valid)/batch_size)\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "def main():\n",
    "    \"\"\" The main steps to train a seq2seq model:\n",
    "    1. Read dataset\n",
    "    2. Preproces each sequnce (create standarized sequnces)\n",
    "        a. Change QID, CONDITION and OUTPUT text to lowercas\n",
    "        b. split QID, CONDITION and OUTPUT text into tokens (words)\n",
    "        c. Replace QID tokens in each sample with standrized tokens (i.e., <QID0>, <QID1>, ...)\n",
    "        d. Replace digit tokens in each sample with standarized tokens (i.e., <DGT0>, <DGT1>, ...)\n",
    "        e. Create standardization dictionary for each sample\n",
    "        f. Add special tokens <BOS> and <EOS> to the begining and end of each sequence\n",
    "    3. Create dictinries to convert input and target sequnces to an integer id\n",
    "    4. Replace input and outpu sequnce tokens with an integre id\n",
    "    5. Pad sequnces with zero to create fixed size input and target sequnces\n",
    "        a. Input sequnce is pre-padded with zero\n",
    "        b. Target sequnce is post-padded \n",
    "    4. Create a seq2seq model\n",
    "    4. Train the model\n",
    "    5. Save the model and model metadata (inclding dictionaries to conver words to id)\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data_path = MT_TRAINING_CORPUS_PATH\n",
    "\n",
    "\n",
    "    # Read dataset from Excel file\n",
    "    qids_raw, conditions_raw, output_raw = read_data(train_data_path)\n",
    "    print(\"\\nLoaded train data set from [{}]\\n\".format(train_data_path))\n",
    "\n",
    "    # Preprocess the raw input text data\n",
    "    _, conditions, outputs, dictionaries_lemanization = prepare_data(qids_raw, conditions_raw, output_raw)\n",
    "    \n",
    "    # Create dictionaries to convert between word and an integer id\n",
    "    # for conditions (Human Longuage) and ouputs (Machine longuage)\n",
    "    condition_word2id, condition_id2word = create_vocabulary(conditions, encoder_vocab_size)\n",
    "    output_word2id, output_id2word = create_vocabulary(outputs, decoder_vocab_size)\n",
    "    \n",
    "    # Replace words of condition and ouput with corresponding id in dictonaries\n",
    "    conditions = replace_using_dict(conditions, condition_word2id, drop_unknown=True)\n",
    "    outputs    = replace_using_dict(outputs, output_word2id, drop_unknown=True)\n",
    "\n",
    "    # Fix all sequnces length to a fixed size with padding\n",
    "    conditions = pad_with_zero(conditions, encoder_seq_length,'pre')\n",
    "    outputs    = pad_with_zero(outputs, decoder_seq_length+1,'post')\n",
    "\n",
    "    # Split train data into train and validation sets\n",
    "    conditions_train, conditions_valid, outputs_train, outputs_valid = train_test_split(conditions, outputs, test_size=validation_size, random_state=42)\n",
    "\n",
    "    # Created a seq2seq Recurrent Neural Network model\n",
    "    model = create_seq2seq_model(encoder_vocab_size, decoder_vocab_size, num_latent_dim)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the seq2seq model\n",
    "    model = train_seq2seq_model(model, conditions_train, conditions_valid, outputs_train, outputs_valid, num_epochs)\n",
    "\n",
    "    \n",
    "    print(\"\\nTrained seq2seq model saved in [{}]\\n\".format(MT_SEQ2SEQ_MODEL_PATH))\n",
    "    \n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded a trained seq2seq model from [./model/mt_seq2seq_model.h5]\n",
      "\n",
      "\n",
      "Loaded test dataset from [./data/MT_test_submission.xlsx]\n",
      "\n",
      "Sample index 0\n",
      "QID:  1010\n",
      "CONDITION:  Terminate if respondent selected ‘A4’ for all 3 product types\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  1010.any(3) \n",
      "\n",
      "\n",
      "Sample index 10\n",
      "QID:  Q16A,QD\n",
      "CONDITION:  ASK ONLY IF QD = 1-4 (ORDERED ANY TEST SALAD)\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qd.between(1:4) \n",
      "\n",
      "\n",
      "Sample index 20\n",
      "QID:  QR8,Q30,QR7\n",
      "CONDITION:  ASK IF Q30=1 AND QR7=1-4\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qr7.any(1)&qr7.any(1) \n",
      "\n",
      "\n",
      "Sample index 30\n",
      "QID:  8014\n",
      "CONDITION:  END Younger than 18 years\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  8014<18 \n",
      "\n",
      "\n",
      "Sample index 40\n",
      "QID:  Q26B,QE2\n",
      "CONDITION:  ASK ONLY IF QE2=1\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qe2.any(1) \n",
      "\n",
      "\n",
      "Sample index 50\n",
      "QID:  Q3,Q2\n",
      "CONDITION:  ASK IF Q2=1\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q2.any(1) \n",
      "\n",
      "\n",
      "Sample index 60\n",
      "QID:  Q17b,QD\n",
      "CONDITION:  ASK ONLY IF QD = 1 (ORDERED BIG TEX TACO SALAD)\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qd.any(1) \n",
      "\n",
      "\n",
      "Sample index 70\n",
      "QID:  8000\n",
      "CONDITION:  Screen out if NOT 2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  8000.notany(2) \n",
      "\n",
      "\n",
      "Sample index 80\n",
      "QID:  Q25B,QE2\n",
      "CONDITION:  ASK ONLY IF QE2=1\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qe2.any(1) \n",
      "\n",
      "\n",
      "Sample index 90\n",
      "QID:  22015,22011\n",
      "CONDITION:  ASK IF CODED 2 AT 22011 \n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  22011.any(2) \n",
      "\n",
      "\n",
      "Sample index 100\n",
      "QID:  Q3A,QA\n",
      "CONDITION:  ASK IF QA = 1 (WOMEN)\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qa.any(1) \n",
      "\n",
      "\n",
      "Sample index 110\n",
      "QID:  6223,Q6215\n",
      "CONDITION:  Instructions: only show if HCP prescribed or recommended one brand = Q6215 A1 OR A2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q6215.any(<DGT0>) \n",
      "\n",
      "\n",
      "Sample index 120\n",
      "QID:  Q4654,Q4200\n",
      "CONDITION:  ROUTING IF: respondent answers 2 @q4200\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q4200.any(2) \n",
      "\n",
      "\n",
      "Sample index 130\n",
      "QID:  1011,1010\n",
      "CONDITION:  Ask this question only to those select ‘A2’ or ‘A4’ for ALL product types at 1010\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  1010.any(<DGT0>) \n",
      "\n",
      "\n",
      "Sample index 140\n",
      "QID:  20043,20033\n",
      "CONDITION:  ROUTING: A1/A2/A3/A4/A5 IN 20033\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  20033.any(<DGT0>,<DGT1>,<DGT2>) \n",
      "\n",
      "\n",
      "Sample index 150\n",
      "QID:  Q25C,Q25B\n",
      "CONDITION:  ASK ONLY IF Q25B=1 \n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q25b.any(1) \n",
      "\n",
      "\n",
      "Sample index 160\n",
      "QID:  Q306,Q305\n",
      "CONDITION:  Q306 IF NOT 6 IN Q305\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q305.notany(6) \n",
      "\n",
      "\n",
      "Sample index 170\n",
      "QID:  D14,D13,D15\n",
      "CONDITION:  [PN : IF D13=99, GO TO D15]\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  d15.any(99) \n",
      "\n",
      "\n",
      "Sample index 180\n",
      "QID:  41962,4006\n",
      "CONDITION:  Ask to those coded 2 in 4006\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  4006.any(2) \n",
      "\n",
      "\n",
      "Sample index 190\n",
      "QID:  QD\n",
      "CONDITION:  QUESTION : DROP DOWN LIST IF QD = 52\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qd.any(52) \n",
      "\n",
      "\n",
      "Sample index 200\n",
      "QID:  Q226,Q225\n",
      "CONDITION:  ASK Q226 IF NOT 5 IN Q225\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q225.notany(5) \n",
      "\n",
      "\n",
      "Sample index 210\n",
      "QID:  Q1004\n",
      "CONDITION:  Screenout if respondent is an experienced mum: Q1004 A2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1004.any(<DGT0>) \n",
      "\n",
      "\n",
      "Sample index 220\n",
      "QID:  QF1,QA1\n",
      "CONDITION:  ASK IF QA1 = 2 ( FRANCE ) IF QF1 = 1\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qa1.any(2) \n",
      "\n",
      "\n",
      "Sample index 230\n",
      "QID:  Q1011\n",
      "CONDITION:  Screenout if not started in past three months: If not Q1011 A1 OR A2 OR A3 OR A4\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1011.notany(<DGT0>,<DGT1>) \n",
      "\n",
      "\n",
      "Sample index 240\n",
      "QID:  3006,30041\n",
      "CONDITION:  SHOW IF  Q30041=1 or 2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  30041.any(1,2) \n",
      "\n",
      "\n",
      "Sample index 250\n",
      "QID:  Q58a,Q57\n",
      "CONDITION:  ASK Q58a IF 1 SELECTED IN Q57.\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q57.any(1) \n",
      "\n",
      "\n",
      "Sample index 260\n",
      "QID:  Q12,Q8\n",
      "CONDITION:  ASK Q12 IF AT LEAST ONE ANSWER AT Q8\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q8.answered() \n",
      "\n",
      "\n",
      "Sample index 270\n",
      "QID:  Q4a,Q1,Q2,Q4\n",
      "CONDITION:  ASK Q4a IF DID NOT SELECT 7 AT Q1 AND Q2 OR 1 SELECTED AT Q4\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1.notany(7)or q2.between.between(1 \n",
      "\n",
      "\n",
      "Sample index 280\n",
      "QID:  Question2\n",
      "CONDITION:  Screenout if Age DOES NOT EQUAL 18-75\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  question2.notbetween(18:75) \n",
      "\n",
      "\n",
      "Sample index 290\n",
      "QID:  Q.CR\n",
      "CONDITION:  PROGRAMMER:  RESPONDENT MUST BE QCR -1 OR -2 TO CONTINUE. DO NOT TERM RESPONDENT.  ALLOW PROGRAM TO CONTINUE.\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q.between(1:2) \n",
      "\n",
      "\n",
      "Sample index 300\n",
      "QID:  8010\n",
      "CONDITION:  SCREEN OUT (under 18 and above 65)\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  8010.notany(18) \n",
      "\n",
      "\n",
      "Sample index 310\n",
      "QID:  Q4,Q2\n",
      "CONDITION:  ASK Q4 IF 7 NOT SELECTED AT Q2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q2.notany(7) \n",
      "\n",
      "\n",
      "Sample index 320\n",
      "QID:  Q54,Q53\n",
      "CONDITION:  ASK IF Q53=2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q53.any(2) \n",
      "\n",
      "\n",
      "Sample index 330\n",
      "QID:  S2\n",
      "CONDITION:  Exclude IF AGE<18 OR >55\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  s2 <18 or s2>55 \n",
      "\n",
      "\n",
      "Sample index 340\n",
      "QID:  1023,1300,1301\n",
      "CONDITION:  SOCCER LOVER IF SELECTED 4 or 5 at 1300 OR 1 or 2 at 1301\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  1300.any(4,5,1) \n",
      "\n",
      "\n",
      "Sample index 350\n",
      "QID:  4056,4055\n",
      "CONDITION:  Ask  4056 if 1 at 4055\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  4055.any(1) \n",
      "\n",
      "\n",
      "Sample index 360\n",
      "QID:  Q17A,QD\n",
      "CONDITION:  ASK ONLY IF QD = 1 \n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qd.any(1) \n",
      "\n",
      "\n",
      "Sample index 370\n",
      "QID:  QG2,QG1\n",
      "CONDITION:  ASK QG2 IF QG1 = 1\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qg1.any(1) \n",
      "\n",
      "\n",
      "Sample index 380\n",
      "QID:  Q1710\n",
      "CONDITION:  ROUTING IF: respondent is aware of advertising of Heineken (Q1700=A1)\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  .any(<DGT0>) \n",
      "\n",
      "\n",
      "Sample index 390\n",
      "QID:  Q1711,Q1700\n",
      "CONDITION:  ROUTING IF: respondent can (partially) remember what Samsung wanted to make clear(Q1710=A1/A2)\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  .any(<DGT0>,<DGT1>) \n",
      "\n",
      "\n",
      "Sample index 400\n",
      "QID:  2205,22017\n",
      "CONDITION:  Ask if coded 1 in 22017\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  22017.any(1) \n",
      "\n",
      "\n",
      "Sample index 410\n",
      "QID:  8010\n",
      "CONDITION:  [Screen out if younger than 18 OR 64 or older]\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  8010<18 or 8010>64 \n",
      "\n",
      "\n",
      "Sample index 420\n",
      "QID:  Q1014,Q1002,Q1003\n",
      "CONDITION:  ROUTING: Q1002–A1 OR Q1003–A2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1002.any(<DGT0>)or q1003.any(<DGT1>) \n",
      "\n",
      "\n",
      "Sample index 430\n",
      "QID:  S7\n",
      "CONDITION:  Terminate at end of screener if <75%\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  s7.any(75) \n",
      "\n",
      "\n",
      "Sample index 440\n",
      "QID:  Q80:Q82,Q79\n",
      "CONDITION:  ASK Q80 TO Q82 IF 2 OR 3 AT Q79\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q82.any(2,3) \n",
      "\n",
      "\n",
      "Sample index 450\n",
      "QID:  8012\n",
      "CONDITION:  Younger than 18 years Screen out\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  8012<18 \n",
      "\n",
      "\n",
      "Sample index 460\n",
      "QID:  QC,QA1\n",
      "CONDITION:  ASK IF QA1 = 1 OR 2 (US/CANADA)\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qa1.any(1,2) \n",
      "\n",
      "\n",
      "Sample index 470\n",
      "QID:  QA2,QA1\n",
      "CONDITION:  ASK IF QA1 = 2 . PROGRAMMER : CONTINUE REMAINDER OF QUESTIONNAIRE IN THE SELECTED LANGUAGE .\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  <QID2>.any(2) \n",
      "\n",
      "\n",
      "Sample index 480\n",
      "QID:  1009,Q1008\n",
      "CONDITION:  Ask only to those who selected 1 in Q1008\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1008.any(1) \n",
      "\n",
      "\n",
      "Sample index 490\n",
      "QID:  QD,QA1\n",
      "CONDITION:  ASK IF QA1 = 1\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  qa1.any(1) \n",
      "\n",
      "\n",
      "Sample index 500\n",
      "QID:  3302,Q3260\n",
      "CONDITION:  IF Q3260, A2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q3260.any(<DGT0>) \n",
      "\n",
      "\n",
      "Sample index 510\n",
      "QID:  Q100103,Q1002,Q1003\n",
      "CONDITION:  ROUTING: Q1002–A1 OR Q1003–A2\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1002.any(<DGT0>)or q1003.any(<DGT1>) \n",
      "\n",
      "\n",
      "Sample index 520\n",
      "QID:  1008\n",
      "CONDITION:  IF RESPONDENT DOES NOT USE THESE SERVICES CODE 7 IN 1008  Thank & End\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  1008.notany(7) \n",
      "\n",
      "\n",
      "Sample index 530\n",
      "QID:  4052a,40522\n",
      "CONDITION:  Ask 4052a if 40522 = 4 or 5\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  40522.any(4,5) \n",
      "\n",
      "\n",
      "Sample index 540\n",
      "QID:  1012,Q1002\n",
      "CONDITION:  IF BOUGHT IPL (Q1002, A2)\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1002.any(<DGT0>) \n",
      "\n",
      "\n",
      "Sample index 550\n",
      "QID:  S5\n",
      "CONDITION:  IF <2 OR >35 TERMINATE\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  s5.any(2,35) \n",
      "\n",
      "\n",
      "Sample index 560\n",
      "QID:  Q5552,Q1002,Q1004\n",
      "CONDITION:  ROUTING: Q1002–A2 OR Q1003–A1\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1002.any(<DGT0>)or q1003.any(<DGT1>) \n",
      "\n",
      "\n",
      "Sample index 570\n",
      "QID:  Q585801,Q1002,Q1008\n",
      "CONDITION:  ROUTING: Q1002–A2 OR Q1003–A1\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q1002.any(<DGT0>)or q1003.any(<DGT1>) \n",
      "\n",
      "\n",
      "Sample index 580\n",
      "QID:  Q5\n",
      "CONDITION:  ROUTING IF: respondent has used at least 1 product\n",
      "OUTPUT:  nan \n",
      "\n",
      "Predicted OUTPUT:  q5.notanswered(1) \n",
      "\n",
      "\n",
      "\n",
      "Saved predictions to [./data/MT_test_submission_with_predcitions.xlsx]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file reads and preproces the test dataset. Loades a trained seq2seq model\n",
    "and predict the iput for each sample in test dataset. It writes prediction results\n",
    "to a file and print to shell\n",
    "\"\"\"\n",
    "\n",
    "### Import required packages\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tensorflow.python import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "MT_TEST_CORPUS_PATH                 = \"./data/MT_test_submission.xlsx\"\n",
    "MT_TEST_CORPUS_PATH_WITH_PREDCITION = \"./data/MT_test_submission_with_predcitions.xlsx\"\n",
    "\n",
    "## Import helper functions and constant\n",
    "\n",
    "## Specify prediction paramets\n",
    "# Beam serahc paramets to predict the most likely target sequence\n",
    "beam_search_max_branch = 3 # Maximum number of branch at each time step for beam search\n",
    "beam_search_max_depth = 4  # Maimum sequnce step to branch in beam search\n",
    "\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model, word2id, id2word):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_input = np.zeros((1, len(input_seq), encoder_vocab_size))\n",
    "    for t, word_id in enumerate(input_seq):\n",
    "        encoder_input[0, t, word_id] = 1\n",
    "\n",
    "    states_value = encoder_model.predict([encoder_input])\n",
    "    # Generate empty target sequence of length 1.\n",
    "    decoder_input = np.zeros((1, 1, decoder_vocab_size))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    decoder_input[0, 0, word2id['<BOS>'] ] = 1 \n",
    "    seq_length = 0\n",
    "    sampled_seq, sampled_seq_prob, sampled_seq_length = decode_sequence_beam(decoder_model, decoder_input, states_value, word2id, seq_length)\n",
    "    \n",
    "    return sampled_seq, sampled_seq_prob\n",
    "\n",
    "def decode_sequence_beam(decoder_model, decoder_input, states_value, word2id, seq_length):\n",
    "    \n",
    "    output_tokens, h, c = decoder_model.predict([decoder_input] + states_value)\n",
    "    states_value = [h, c]\n",
    "    \n",
    "    seq_length += 1\n",
    "    # Sample a token\n",
    "    if seq_length < beam_search_max_depth:\n",
    "        number_search_branch = beam_search_max_branch\n",
    "    else:\n",
    "        number_search_branch = 1\n",
    "    \n",
    "    beam_top_token_indecies = np.argsort(output_tokens[0, -1, :])[-number_search_branch:]\n",
    "    sampled_seq_list = []\n",
    "    sampled_seq_prob_list = []\n",
    "    sampled_seq_length_list = []\n",
    "    for beam in range(number_search_branch):\n",
    "        sampled_token_index = beam_top_token_indecies[beam]\n",
    "        sampled_token_prob  = output_tokens[0, -1, sampled_token_index]\n",
    "        if sampled_token_index == word2id['<EOS>'] or seq_length == decoder_seq_length:\n",
    "            return [sampled_token_index,0], sampled_token_prob, 0.00000001\n",
    "        else:\n",
    "            # Update the target sequence (of length 1).\n",
    "            decoder_input = np.zeros((1, 1, decoder_vocab_size))\n",
    "            decoder_input[0, 0, sampled_token_index] = 1\n",
    "            # Update states\n",
    "            sampled_seq, sampled_seq_prob, sampled_seq_length = decode_sequence_beam(decoder_model, decoder_input, states_value, word2id, seq_length)\n",
    "            sampled_seq.append(sampled_token_index)\n",
    "            sampled_seq_prob *= sampled_token_prob\n",
    "            \n",
    "            sampled_seq_list.append(sampled_seq)\n",
    "            sampled_seq_prob_list.append(sampled_seq_prob)\n",
    "            sampled_seq_length_list.append(sampled_seq_length)\n",
    "    \n",
    "    weighted_prob = np.log(np.array(sampled_seq_prob_list))/np.array(sampled_seq_length_list)\n",
    "    \n",
    "    best_beam = np.argmax(weighted_prob)\n",
    "    \n",
    "    return sampled_seq_list[best_beam], sampled_seq_prob_list[best_beam], sampled_seq_length_list[best_beam]+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # construct the argument parser and parse the arguments\n",
    "    test_data_path = MT_TEST_CORPUS_PATH\n",
    "    test_data_output_path = MT_TEST_CORPUS_PATH_WITH_PREDCITION\n",
    "\n",
    "    # Load model and metadata\n",
    "    model = load_model(MT_SEQ2SEQ_MODEL_PATH)\n",
    "\n",
    "    with open(MT_META_DATA_FILE_PATH,'rb') as f:\n",
    "        [condition_word2id, condition_id2word, output_word2id, output_id2word] = pickle.load(f)\n",
    "\n",
    "    print(\"\\nLoaded a trained seq2seq model from [{}]\\n\".format(MT_SEQ2SEQ_MODEL_PATH))\n",
    "\n",
    "    encoder_model, decoder_model = create_seq2seq_inference_model(model, num_latent_dim)\n",
    "    \n",
    "    #test_data_path = MT_TRAINING_CORPUS_PATH\n",
    "    \n",
    "    #Read dataset from Excel file\n",
    "    qids_raw, conditions_raw, output_raw = read_data(test_data_path)\n",
    "    print(\"\\nLoaded test dataset from [{}]\\n\".format(test_data_path))\n",
    "\n",
    "    # Preprocess the raw input text data\n",
    "    _, conditions, _, dictionaries_lemanization = prepare_data(qids_raw, conditions_raw, output_raw)\n",
    "    \n",
    "    # Replace words of qid, condition and ouput with corresponding id in dictonaries\n",
    "    conditions = replace_using_dict(conditions, condition_word2id, drop_unknown=True)\n",
    "\n",
    "    # Fix all sequnces length to a fixed size with padding\n",
    "    conditions = pad_with_zero(conditions, encoder_seq_length,'pre')\n",
    "\n",
    "    outputs_predcited = [None for _ in conditions]\n",
    "    for sample_index, condition in enumerate(conditions):\n",
    "\n",
    "        input_seq = condition\n",
    "        decoded_seqeunce, _ = decode_sequence(input_seq, encoder_model, decoder_model, output_word2id, output_id2word)\n",
    "        \n",
    "        decoded_seqeunce = replace_using_dict([decoded_seqeunce], output_id2word)\n",
    "        decoded_seqeunce = replace_using_dict(decoded_seqeunce, dictionaries_lemanization[sample_index])\n",
    "\n",
    "        decoded_seqeunce = [seq for seq in decoded_seqeunce[0] if seq != '<PAD>' and seq != '<EOS>']\n",
    "        decoded_seqeunce = reversed(decoded_seqeunce)\n",
    "        decoded_seqeunce = ''.join(decoded_seqeunce)\n",
    "\n",
    "        outputs_predcited[sample_index] = decoded_seqeunce\n",
    "\n",
    "        if sample_index % 10 == 0:\n",
    "            log_to_shell(sample_index, qids_raw[sample_index],\n",
    "                           conditions_raw[sample_index], output_raw[sample_index],\n",
    "                           decoded_seqeunce )\n",
    "        \n",
    "    write_data(qids_raw, conditions_raw, outputs_predcited, test_data_output_path)\n",
    "    print(\"\\nSaved predictions to [{}]\\n\".format(test_data_output_path))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
