{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" tools.py\n",
    "This file provides some helper functions required to read and prepare data\n",
    "for the model\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "SPLIT_PATTERN_WITH_DILIMITER = r'([`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?\\n\\s])\\s*'\n",
    "SPLIT_PATTERN_NO_DILIMITER   = r'[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?\\n\\s]\\s*'\n",
    "\n",
    "\n",
    "def read_data(data_path):\n",
    "    \"\"\" Reads data from an excel file\n",
    "    Args:\n",
    "        data_path: Input data path\n",
    "    Returns:\n",
    "        qids_raw: Pyhon list of raw qid texts\n",
    "        conditions_raw: Pyhon list of raw condition texts\n",
    "        outputs_raw: Pyhon list of raw output texts\n",
    "    \"\"\"\n",
    "    data_set = pd.read_excel(data_path)\n",
    "    qids_raw       = data_set[\"QID\"].values\n",
    "    conditions_raw = data_set[\"CONDITION\"].values\n",
    "    outputs_raw    = data_set[\"OUTPUT\"].values\n",
    "    return qids_raw, conditions_raw, outputs_raw\n",
    "\n",
    "def write_data(qids, conditions, outputs, data_path):\n",
    "    \"\"\" Writes data to an excel file\n",
    "    Args:\n",
    "        qids: Pyhon list of qid texts\n",
    "        conditions: Pyhon list of condition texts\n",
    "        outputs: Pyhon list of output texts\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data_set = pd.DataFrame(list(zip(qids, conditions, outputs)),\n",
    "                            columns=[\"QID\", \"CONDITION\", \"OUTPUT\"])\n",
    "    data_set.to_excel(data_path)\n",
    "\n",
    "\n",
    "def prepare_data(qids_raw, conditions_raw, outputs_raw):\n",
    "    \"\"\" Prepares data for the model\n",
    "    Args:\n",
    "        qids_raw: Pyhon list of raw qid texts\n",
    "        conditions_raw: Pyhon list of raw condition texts\n",
    "        outputs_raw: Pyhon list of raw output texts\n",
    "    Returns:\n",
    "        qids: Pyhon list of preprocessed qid sequnces\n",
    "        conditions: Pyhon list of preprocessed condition sequnces\n",
    "        outputs: Pyhon list of preprocessed output sequnces\n",
    "        dictionaries_standardization: Pyhton list of dictionaries used for standardizing samples\n",
    "    \"\"\"\n",
    "\n",
    "    qids = []\n",
    "    conditions = []\n",
    "    outputs = []\n",
    "    dictionaries_standardization = []\n",
    "    for qid_raw, condition_raw, output_raw in zip(qids_raw, conditions_raw, outputs_raw):\n",
    "        qid, condition, output, dictionary = preprocess_sample(qid_raw, condition_raw, output_raw)\n",
    "        qids.append(qid)\n",
    "        conditions.append(condition)\n",
    "        outputs.append(output)\n",
    "        dictionaries_standardization.append(dictionary)\n",
    "\n",
    "    return qids, conditions, outputs, dictionaries_standardization\n",
    "\n",
    "def preprocess_sample(qid_raw, condition_raw, output_raw):\n",
    "    \"\"\" Preproces a sample to create standarized sequnces\n",
    "        a. Change qid_raw, condition_raw and output_raw text to lowercas\n",
    "        b. split qid_raw, condition_raw and output_raw text into tokens (words)\n",
    "        c. Replace qid_raw tokens with standrized tokens (i.e., <QID0>, <QID1>, ...)\n",
    "        d. Replace digit tokens with standarized tokens (i.e., <DGT0>, <DGT1>, ...)\n",
    "        e. Create standardization dictionary for each sample\n",
    "        f. Add special tokens <BOS> and <EOS> to the begining and end of each sequence\n",
    "    \"\"\"\n",
    "    qid, condition, output = split_to_words(qid_raw, condition_raw, output_raw)\n",
    "    \n",
    "    qid, condition, output, dictionary_standardization = standardize_words(qid, condition, output)\n",
    "\n",
    "    return qid, condition, output, dictionary_standardization\n",
    "\n",
    "def split_to_words(qid_raw, condition_raw, output_raw):\n",
    "    \"\"\" Splits input raw texts into words (tokens)\n",
    "    Args:\n",
    "        qid_raw: raw qid text\n",
    "        condition_raw: Pyhon list of raw condition texts\n",
    "        output_raw: raw output texts\n",
    "    Return:\n",
    "        qid: Python array of qid words (tokens)\n",
    "        condition: Python array of condition words (tokens)\n",
    "        output: Python array of output words (tokens)\n",
    "    \"\"\"\n",
    "    qid       = re.split(SPLIT_PATTERN_NO_DILIMITER, str(qid_raw))\n",
    "    condition = re.split(SPLIT_PATTERN_NO_DILIMITER, str(condition_raw))\n",
    "    condition = [cond for cond in condition if cond != \" \" and cond != \"\"]\n",
    "    output    = re.split(SPLIT_PATTERN_WITH_DILIMITER, str(output_raw))\n",
    "\n",
    "    qid       = [x.lower() for x in qid]\n",
    "    condition = [x.lower() for x in condition]\n",
    "    output    = [x.lower() for x in output]\n",
    "    \n",
    "    return qid, condition, output\n",
    "\n",
    "def standardize_words(qid, condition, output):\n",
    "    \"\"\" Standarizes a sample by replacing qids and digits with stanadard words\n",
    "    Args:\n",
    "        qid: Python array of qid words (tokens)\n",
    "        condition: Python array of condition words (tokens)\n",
    "        output: Python array of output words (tokens)\n",
    "    Retursn:\n",
    "        qid: Python array of standarized qid words (tokens)\n",
    "        condition: Python array of standarized condition words (tokens)\n",
    "        output: Python array of standarized output words (tokens)\n",
    "        dictionary_standardization: Pyhton dictionary used for standardizing sample\n",
    "    \"\"\"\n",
    "    dictionary_standardization = {}\n",
    "    for index, id in enumerate(qid):\n",
    "        standard_qid = '<QID{}>'.format(index)\n",
    "        dictionary_standardization[standard_qid] = qid[index]\n",
    "        qid[index] = standard_qid\n",
    "    \n",
    "        for word_index in range(len(condition)):\n",
    "            if condition[word_index] == id:\n",
    "                condition[word_index] = standard_qid\n",
    "\n",
    "        for word_index in range(len(output)):\n",
    "            if output[word_index] == id:\n",
    "                output[word_index] = standard_qid\n",
    "\n",
    "    digit_num = 0\n",
    "    for word in condition:\n",
    "        if word.isdigit():\n",
    "            standard_digit = '<DGT{}>'.format(digit_num)\n",
    "            digit_num += 1\n",
    "            dictionary_standardization[standard_digit] = word\n",
    "\n",
    "            for word_index in range(len(condition)):\n",
    "                if condition[word_index] == word:\n",
    "                    condition[word_index] = standard_digit\n",
    "\n",
    "            for word_index in range(len(output)):\n",
    "                if output[word_index] == word:\n",
    "                    output[word_index] = standard_digit\n",
    "\n",
    "    for word in output:\n",
    "        if word.isdigit():\n",
    "            standard_digit = '<DGT{}>'.format(digit_num)\n",
    "            digit_num += 1\n",
    "            dictionary_standardization[standard_digit] = word\n",
    "            for word_index in range(len(output)):\n",
    "                if output[word_index] == word:\n",
    "                    output[word_index] = standard_digit\n",
    "    \n",
    "    condition   = ['<BOS>']  + condition + ['<EOS>']\n",
    "    output      = ['<BOS>']  + output  + ['<EOS>']\n",
    "\n",
    "    return qid, condition, output, dictionary_standardization\n",
    "\n",
    "\n",
    "def create_vocabulary(word_list, max_vocab_size):\n",
    "    \"\"\" Create Vocabulary dictionary\n",
    "    Args:\n",
    "        text(str): inout word list\n",
    "        max_vocab_size: maximum number of words in the vocabulary\n",
    "    Returns:\n",
    "        word2id(dict): word to id mapping\n",
    "        id2word(dict): id to word mapping\n",
    "    \"\"\"\n",
    "    words = [word for sample in word_list for word in sample]\n",
    "    freq = Counter(words)\n",
    "    word2id = {'<PAD>' : 0}\n",
    "    id2word = {0 : '<PAD>'}\n",
    "\n",
    "    for word, _ in freq.most_common():\n",
    "        id = len(word2id)\n",
    "        if word not in word2id:\n",
    "            word2id[word] = id\n",
    "            id2word[id] = word\n",
    "        if id == max_vocab_size - 1 :\n",
    "            break\n",
    "\n",
    "    return word2id, id2word\n",
    "\n",
    "\n",
    "def replace_using_dict(list, dictionary, drop_unknown=False):\n",
    "    \"\"\" Replaces tokens of the input list using a dictionary\n",
    "    Args:\n",
    "        list: a python list of word sequences\n",
    "        dictionary: a dictionary to convert tokens\n",
    "        drop_unknown: a flag to specify whether keep or drop tokens not in dictionary\n",
    "    Returns:\n",
    "        replaced_list: replaced Pyhthon list of word sequences \n",
    "\n",
    "    \"\"\"\n",
    "    replaced_list = []\n",
    "    for line in list:\n",
    "        if drop_unknown:\n",
    "            translated_line = [dictionary[word] for word in line if word in dictionary]\n",
    "        else:\n",
    "            translated_line = [dictionary[word] if word in dictionary else word for word in line]\n",
    "        replaced_list.append(translated_line)\n",
    "    \n",
    "    return replaced_list\n",
    "\n",
    "def pad_with_zero(list, max_length, pad_type):\n",
    "    \"\"\" Pad sequnces in the input list with zero\n",
    "    Args:\n",
    "        list: a Python list of word sequnces\n",
    "        max_length: maximum length of each sequence\n",
    "        pad_type: whether pad begining or end of the sequnces\n",
    "    Return:\n",
    "        padded_list: padded list of word sequnces\n",
    "    \"\"\"\n",
    "    padded_list = pad_sequences(list, maxlen=max_length, padding=pad_type, truncating='post')\n",
    "    return padded_list\n",
    "\n",
    "\n",
    "def log_to_shell(index, qid_raw, condition_raw, output_raw, decoded_seqeunce):\n",
    "    \"\"\" Prints information to shell\n",
    "    Args:\n",
    "        qid_raw: raw qid text\n",
    "        condition_raw: Pyhon list of raw condition texts\n",
    "        output_raw: raw output texts\n",
    "        decoded_seqeunce: decoded output sequnce\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Sample index\",       index)\n",
    "    print(\"QID: \",              qid_raw)\n",
    "    print(\"CONDITION: \",        condition_raw)\n",
    "    print(\"OUTPUT: \",           output_raw,'\\n')\n",
    "    print(\"Predicted OUTPUT: \", decoded_seqeunce, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded train data set from [./data/MT_training_corpus.xlsx]\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None, 150)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 50)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 40), (None,  30560       encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 40), ( 14560       decoder_input[0][0]              \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 50)     2050        decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 47,170\n",
      "Trainable params: 47,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      " - 5s - loss: 1.7983 - acc: 0.6187 - val_loss: 1.1091 - val_acc: 0.8003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm_2/while/Exit_2:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'encoder_lstm_2/while/Exit_3:0' shape=(?, 40) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      " - 3s - loss: 0.8477 - acc: 0.8229 - val_loss: 0.5956 - val_acc: 0.8608\n",
      "Epoch 3/100\n",
      " - 3s - loss: 0.4827 - acc: 0.8812 - val_loss: 0.5890 - val_acc: 0.8381\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.3212 - acc: 0.9216 - val_loss: 0.5310 - val_acc: 0.8700\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.2829 - acc: 0.9307 - val_loss: 0.4132 - val_acc: 0.8894\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.2204 - acc: 0.9453 - val_loss: 0.4135 - val_acc: 0.9053\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.2026 - acc: 0.9507 - val_loss: 0.2861 - val_acc: 0.9289\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.1434 - acc: 0.9673 - val_loss: 0.2717 - val_acc: 0.9397\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.1733 - acc: 0.9559 - val_loss: 0.2833 - val_acc: 0.9303\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.1157 - acc: 0.9770 - val_loss: 0.2346 - val_acc: 0.9450\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.1368 - acc: 0.9656 - val_loss: 0.1926 - val_acc: 0.9533\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.1382 - acc: 0.9697 - val_loss: 0.1332 - val_acc: 0.9631\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.0889 - acc: 0.9806 - val_loss: 0.1107 - val_acc: 0.9611\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.0685 - acc: 0.9854 - val_loss: 0.1256 - val_acc: 0.9664\n",
      "Epoch 15/100\n",
      " - 2s - loss: 0.0580 - acc: 0.9869 - val_loss: 0.1689 - val_acc: 0.9689\n",
      "Epoch 16/100\n",
      " - 2s - loss: 0.0729 - acc: 0.9840 - val_loss: 0.1887 - val_acc: 0.9486\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.0457 - acc: 0.9912 - val_loss: 0.2065 - val_acc: 0.9542\n",
      "Epoch 18/100\n",
      " - 2s - loss: 0.0451 - acc: 0.9905 - val_loss: 0.1328 - val_acc: 0.9708\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.0588 - acc: 0.9881 - val_loss: 0.1234 - val_acc: 0.9625\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.0570 - acc: 0.9873 - val_loss: 0.2060 - val_acc: 0.9408\n",
      "Epoch 21/100\n",
      " - 2s - loss: 0.0658 - acc: 0.9869 - val_loss: 0.2286 - val_acc: 0.9550\n",
      "Epoch 22/100\n",
      " - 2s - loss: 0.0497 - acc: 0.9881 - val_loss: 0.2604 - val_acc: 0.9472\n",
      "Epoch 23/100\n",
      " - 2s - loss: 0.0603 - acc: 0.9864 - val_loss: 0.2422 - val_acc: 0.9286\n",
      "Epoch 24/100\n",
      " - 2s - loss: 0.0355 - acc: 0.9924 - val_loss: 0.2066 - val_acc: 0.9553\n",
      "Epoch 25/100\n",
      " - 2s - loss: 0.0327 - acc: 0.9925 - val_loss: 0.2423 - val_acc: 0.9486\n",
      "Epoch 26/100\n",
      " - 2s - loss: 0.0249 - acc: 0.9947 - val_loss: 0.2055 - val_acc: 0.9689\n",
      "Epoch 27/100\n",
      " - 2s - loss: 0.0210 - acc: 0.9961 - val_loss: 0.1346 - val_acc: 0.9703\n",
      "Epoch 28/100\n",
      " - 2s - loss: 0.0587 - acc: 0.9890 - val_loss: 0.2633 - val_acc: 0.9308\n",
      "Epoch 29/100\n",
      " - 2s - loss: 0.0284 - acc: 0.9943 - val_loss: 0.2027 - val_acc: 0.9506\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cc966bac0eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-cc966bac0eeb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# Train the seq2seq model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_seq2seq_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditions_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditions_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;31m# Save model and metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cc966bac0eeb>\u001b[0m in \u001b[0;36mtrain_seq2seq_model\u001b[0;34m(model, X_train, X_valid, y_train, y_valid, epochs)\u001b[0m\n\u001b[1;32m    198\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                         validation_steps=len(X_valid)/batch_size)\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamal/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamal/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamal/.local/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamal/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamal/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamal/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamal/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\" train.py\n",
    "This file reads and preproces the train dataset and builds a seq2seq model\n",
    "using Recurrent Neurak Networks to predict a target sequnce from an input sequnce.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "### Import required packages\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "## Import helper functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tools import read_data, prepare_data, create_vocabulary, replace_using_dict, pad_with_zero\n",
    "\n",
    "## Define default training data path\n",
    "MT_TRAINING_CORPUS_PATH  = \"./data/MT_training_corpus.xlsx\"\n",
    "\n",
    "## Specify path to save model and metadata\n",
    "MT_SEQ2SEQ_MODEL_PATH    = \"./model/mt_seq2seq_model.h5\"\n",
    "MT_MODEL_CHECKPOINT_PATH = \"./model/model.chpt\"\n",
    "MT_META_DATA_FILE_PATH   = \"./model/metadata.pickle\"\n",
    "\n",
    "## Define model parameter\n",
    "# Encoder and Decoder maximum vocabulary size\n",
    "encoder_vocab_size = 150\n",
    "decoder_vocab_size = 50\n",
    "\n",
    "# Encoder and Decoder sequnces length\n",
    "encoder_seq_length = 20\n",
    "decoder_seq_length = 15\n",
    "\n",
    "# Number of training epcohs\n",
    "num_epochs = 30\n",
    "\n",
    "# Training Batch size\n",
    "batch_size = 20\n",
    "\n",
    "# Number of LSTM latend dimention in both Encoder and Decoder\n",
    "num_latent_dim = 40\n",
    "\n",
    "# Fraction of data used for validation during training the model\n",
    "validation_size = 0.1\n",
    "\n",
    "def data_generator(X, y, batch_size):\n",
    "    \"\"\" Creates a data genrator to feed encoder and decoder input sequnces and decoder\n",
    "    target sequnce\n",
    "    Args:\n",
    "        X: input sequnces\n",
    "        y: target sequnces\n",
    "    Returns:\n",
    "         yields a batch of encoder and decoder input sequnces and decoder target sequnce\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for j in range(random.randint(1,len(X)-batch_size)):\n",
    "            encoder_input_sequnce  = np.zeros((batch_size, encoder_seq_length, encoder_vocab_size), dtype='float32')\n",
    "            decoder_input_sequnce  = np.zeros((batch_size, decoder_seq_length, decoder_vocab_size), dtype='float32')\n",
    "            decoder_target_sequnce = np.zeros((batch_size, decoder_seq_length, decoder_vocab_size), dtype='float32')\n",
    "\n",
    "            for i, (input_seq, target_seq) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_seq):\n",
    "                    encoder_input_sequnce[i, t, word] = 1  # encoder input seq\n",
    "                for t, word in enumerate(target_seq):\n",
    "                    if t < decoder_seq_length:\n",
    "                        decoder_input_sequnce[i, t, word] = 1 # decoder input seq\n",
    "                    if t>0:\n",
    "                        decoder_target_sequnce[i, t-1, word] = 1 # decoder target seq\n",
    "\n",
    "            yield([encoder_input_sequnce, decoder_input_sequnce], decoder_target_sequnce)            \n",
    " \n",
    "\n",
    "def create_seq2seq_model(encoder_vocab_size, decoder_vocab_size, latent_dim):\n",
    "    \"\"\" Creates a seq2seq model using Recurrent Neural Networks(RNN).\n",
    "    The encoder consists of a left-to-right LSTM layer and outputs states to decoder.\n",
    "    The decoder is also consists of a left-to-right LSTM layer and outputs a sequence that\n",
    "    are fed to time distributed fully connected layers with softmax activation to predict \n",
    "    target sequence. \n",
    "    Args:\n",
    "        encoder_vocab_size: number of encoder tokens (i.e., encoder vocab size)\n",
    "        decoder_vocab_size: size of  decoder tokens (i.e., decoder vocab size)\n",
    "        latent_dim: number of LSTM hidden dimenetions\n",
    "    Returns:\n",
    "        model: seq2seq model\n",
    "    \"\"\"\n",
    "\n",
    "    ### Encoder\n",
    "    ## Input layer\n",
    "    encoder_inputs = Input(shape=(None, encoder_vocab_size), name='encoder_input')\n",
    "    ## LSTM layer\n",
    "    encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We keep encoder states and discard encoder ouput.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    ### Decoder\n",
    "    ## Input layer\n",
    "    decoder_inputs = Input(shape=(None, decoder_vocab_size), name='decoder_input')\n",
    "    ## Left to right LSTM layer\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                        initial_state=encoder_states)\n",
    "    ## Fully connected layer\n",
    "    decoder_dense = Dense(decoder_vocab_size, activation='softmax', name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    ### Model to jointly train Encoder and Decoder \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_seq2seq_inference_model(model, latent_dim):\n",
    "    \"\"\" Creates a seq2seq inference model by extracting Encoder and Decoder models\n",
    "     from the input seq2seq model.\n",
    "    Args:\n",
    "        model: a seq2seq model\n",
    "        laten_dim: number of latent dimention of the seq2seq model\n",
    "    Returns:\n",
    "        encoder_model: encoder model of input seq2seq model\n",
    "        decoder_model: decoder model of input seq2seq model\n",
    "    \"\"\"\n",
    "    ### Inference Model\n",
    "    # 1. Encode the input sequence using Encoder and return state for decoder input\n",
    "    # 2. Run one step of decoder with this intial state and \"start of sequnce\" token\n",
    "    #  as input. The output will be used as the next decoder input sequnce token\n",
    "    # 3. This procedure is repteated to predict all output sequnce \n",
    "    \n",
    "    ### Encoder Model\n",
    "    encoder_inputs = model.input[0] \n",
    "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer('encoder_lstm').output   # lstm_1\n",
    "    encoder_states = [state_h_enc, state_c_enc]\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    ### Decoder Model\n",
    "    ## Decoder State Input\n",
    "    decoder_inputs = model.input[1]\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    ## Decoder LSTM layer\n",
    "    decoder_lstm = model.get_layer('decoder_lstm')\n",
    "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h_dec, state_c_dec]\n",
    "    ## Decoder Fully connected layer\n",
    "    decoder_dense = model.get_layer('decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                            [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def train_seq2seq_model(model, X_train, X_valid, y_train, y_valid, epochs):\n",
    "    \"\"\" Compiles and trains the seq2seq model. The train data is fed to model\n",
    "    using a generator function\n",
    "    Args:\n",
    "        model: seq2seq model\n",
    "        X_train: train data input sequnce (conditions)\n",
    "        X_valid: train data input sequnce (conditions)\n",
    "        y_train: validation target sequnce sequnce (ouputs)\n",
    "        y_valid: validation target sequnce (ouputs)\n",
    "        epochs: number of epochs to train model\n",
    "    Returns:\n",
    "        model: trained seq2seq model\n",
    "    \"\"\"\n",
    "\n",
    "    # Model is trainined to minimize cross enthrop between true target sequnce\n",
    "    # and predicted target sequnce\n",
    "    # Optimizer is set to Nadam and accuracy is used as metric\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='Nadam',\n",
    "                    metrics=['acc'])\n",
    "    \n",
    "    # Creats data genrators to feed train and validation data\n",
    "    train_data_generator = data_generator(X_train, y_train, batch_size)\n",
    "    valid_data_generator = data_generator(X_valid, y_valid, batch_size)\n",
    "    \n",
    "    # Define callback fo model checkpoint\n",
    "    callbacks = [ModelCheckpoint(MT_MODEL_CHECKPOINT_PATH, save_best_only=True, save_weights_only=False)]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit_generator(train_data_generator,\n",
    "                        validation_data=valid_data_generator,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=2,\n",
    "                        steps_per_epoch=len(X_train)/batch_size,\n",
    "                        validation_steps=len(X_valid)/batch_size)\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "def main():\n",
    "    \"\"\" The main steps to train a seq2seq model:\n",
    "    1. Read dataset\n",
    "    2. Preproces each sequnce (create standarized sequnces)\n",
    "        a. Change QID, CONDITION and OUTPUT text to lowercase\n",
    "        b. split QID, CONDITION and OUTPUT text into tokens (words)\n",
    "        c. Replace QID tokens in each sample with standrized tokens (i.e., <QID0>, <QID1>, ...)\n",
    "        d. Replace digit tokens in each sample with standarized tokens (i.e., <DGT0>, <DGT1>, ...)\n",
    "        e. Create standardization dictionary for each sample\n",
    "        f. Add special tokens <BOS> and <EOS> to the begining and end of each sequence\n",
    "    3. Create dictinries to convert input and target sequnces to an integer id\n",
    "    4. Replace input and outpu sequnce tokens with an integre id\n",
    "    5. Pad sequnces with zero to create fixed size input and target sequnces\n",
    "        a. Input sequnce is pre-padded with zero\n",
    "        b. Target sequnce is post-padded \n",
    "    4. Create a seq2seq model\n",
    "    4. Train the model\n",
    "    5. Save the model and model metadata (inclding dictionaries to conver words to id)\n",
    "    \"\"\"\n",
    "\n",
    "    # Train data path\n",
    "    train_data_path = MT_TRAINING_CORPUS_PATH\n",
    "\n",
    "    if not os.path.exists(train_data_path):\n",
    "        print(\"\\n Specified train data path [%s] does not exist\\n\" % train_data_path)\n",
    "        return\n",
    "\n",
    "    # Read dataset from Excel file\n",
    "    qids_raw, conditions_raw, output_raw = read_data(train_data_path)\n",
    "    print(\"\\nLoaded train data set from [{}]\\n\".format(train_data_path))\n",
    "\n",
    "    # Preprocess the raw input text data\n",
    "    _, conditions, outputs, dictionaries_lemanization = prepare_data(qids_raw, conditions_raw, output_raw)\n",
    "    \n",
    "    # Create dictionaries to convert between word and an integer id\n",
    "    # for conditions (Human Longuage) and ouputs (Machine longuage)\n",
    "    condition_word2id, condition_id2word = create_vocabulary(conditions, encoder_vocab_size)\n",
    "    output_word2id, output_id2word = create_vocabulary(outputs, decoder_vocab_size)\n",
    "    \n",
    "    # Replace words of condition and ouput with corresponding id in dictonaries\n",
    "    conditions = replace_using_dict(conditions, condition_word2id, drop_unknown=True)\n",
    "    outputs    = replace_using_dict(outputs, output_word2id, drop_unknown=True)\n",
    "\n",
    "    # Fix all sequnces length to a fixed size with padding\n",
    "    conditions = pad_with_zero(conditions, encoder_seq_length,'pre')\n",
    "    outputs    = pad_with_zero(outputs, decoder_seq_length+1,'post')\n",
    "\n",
    "    # Split train data into train and validation sets\n",
    "    conditions_train, conditions_valid, outputs_train, outputs_valid = train_test_split(conditions, outputs, test_size=validation_size, random_state=42)\n",
    "\n",
    "    # Created a seq2seq Recurrent Neural Network model\n",
    "    model = create_seq2seq_model(encoder_vocab_size, decoder_vocab_size, num_latent_dim)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the seq2seq model\n",
    "    model = train_seq2seq_model(model, conditions_train, conditions_valid, outputs_train, outputs_valid, num_epochs)\n",
    "\n",
    "    # Save model and metadata\n",
    "    model.save(MT_SEQ2SEQ_MODEL_PATH)\n",
    "    with open(MT_META_DATA_FILE_PATH,'wb') as f:\n",
    "        pickle.dump([condition_word2id,condition_id2word, output_word2id, output_id2word], f)\n",
    "    \n",
    "    print(\"\\nTrained seq2seq model saved in [{}]\\n\".format(MT_SEQ2SEQ_MODEL_PATH))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" predict.py\n",
    "This file reads and preproces the test dataset. Loades a trained seq2seq model\n",
    "and predict the iput for each sample in test dataset. It writes prediction results\n",
    "to a file and print to shell\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "### Import required packages\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tensorflow.python import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "MT_TEST_CORPUS_PATH                 = \"./data/MT_test_submission.xlsx\"\n",
    "MT_TEST_CORPUS_PATH_WITH_PREDCITION = \"./data/MT_test_submission_with_predcitions.xlsx\"\n",
    "\n",
    "## Import helper functions and constants\n",
    "from tools import read_data, prepare_data, replace_using_dict, pad_with_zero, write_data, log_to_shell\n",
    "from train import MT_SEQ2SEQ_MODEL_PATH, MT_META_DATA_FILE_PATH\n",
    "from train import create_seq2seq_inference_model\n",
    "from train import encoder_seq_length, decoder_seq_length, encoder_vocab_size, decoder_vocab_size, num_latent_dim\n",
    "\n",
    "## Specify prediction paramets\n",
    "# Beam serahc paramets to predict the most likely target sequence\n",
    "beam_search_max_branch = 3 # Maximum number of branch at each time step for beam search\n",
    "beam_search_max_depth = 4  # Maimum sequnce step to branch in beam search\n",
    "\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model, word2id, id2word):\n",
    "    \"\"\" Decodes an input sequnce uing the enoder and decoder model of trained seq2seq model\n",
    "    Beam serahc algorithm is used to find a decoded sequnce with highed liklihood.\n",
    "    Args:\n",
    "        input_seq: Input sequnce\n",
    "        encoder_model: Enoder model of the seq2seq model (Keras)\n",
    "        decoder_model: Decoder model of the seq2seq model (keras)\n",
    "        word2id: Python dictionary to conver word to id\n",
    "        id2word: Python dictionary to conver id to word\n",
    "    Returns:\n",
    "        decoded_seq: Decoded sequence predicted by the model\n",
    "        decoded_seq_prob: The linkleihood of the predicted sequnce by the model\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_input = np.zeros((1, len(input_seq), encoder_vocab_size))\n",
    "    for t, word_id in enumerate(input_seq):\n",
    "        encoder_input[0, t, word_id] = 1\n",
    "\n",
    "    states_value = encoder_model.predict([encoder_input])\n",
    "    # Generate empty target sequence of length 1.\n",
    "    decoder_input = np.zeros((1, 1, decoder_vocab_size))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    decoder_input[0, 0, word2id['<BOS>'] ] = 1 \n",
    "    seq_length = 0\n",
    "    decoded_seq, decoded_seq_prob, _ = decode_sequence_beam(decoder_model, decoder_input, states_value, word2id, seq_length)\n",
    "    \n",
    "    return decoded_seq, decoded_seq_prob\n",
    "\n",
    "def decode_sequence_beam(decoder_model, decoder_input, states_value, word2id, seq_length):\n",
    "    \"\"\" This function decodes a sequnce using beam search. That is in each step of\n",
    "    decoding, search space tree is branched based on the number of specified number_search_branch\n",
    "    parameter for maximum depth of beam_search_max_depth\n",
    "    The beam search algorithm is implemented using a recursive call of this function itself.\n",
    "    Args:\n",
    "        decoder_model: Decoder model of the seq2seq model (keras)\n",
    "        decoder_input: The input to decoder in each step\n",
    "        states_value: The previous state values input\n",
    "        word2id: Python dictionary to conver word to id\n",
    "        seq_length: Current Sequence length from the begining of sequnce (used to control beam search depth)\n",
    "    Returns:\n",
    "        sampled_seq: Sampled sequence upto this step (from end to this step, reursive function call)\n",
    "        sampled_seq_prob: The linklihood of sampled sequnce upto this step (from end to this step)\n",
    "        sampled_seq_length:The sampled Sequnce length to the the end of sequnce (from end to this step)\n",
    "    \"\"\"\n",
    "    ## Get probabilitis of next word in the sequnce and state values\n",
    "    output_tokens, h, c = decoder_model.predict([decoder_input] + states_value)\n",
    "    \n",
    "    ## Update states\n",
    "    states_value = [h, c]\n",
    "    \n",
    "    ## Increment sequence length\n",
    "    seq_length += 1\n",
    "    \n",
    "    ## Choose number of branches to split tree for beam search\n",
    "    # To avoid too many searches will branch up to beam_search_max_depth sequnce length\n",
    "    if seq_length < beam_search_max_depth:\n",
    "        number_search_branch = beam_search_max_branch\n",
    "    else:\n",
    "        number_search_branch = 1\n",
    "    \n",
    "    ## Choose tokens with highest probabities\n",
    "    beam_top_token_indecies = np.argsort(output_tokens[0, -1, :])[-number_search_branch:]\n",
    "    \n",
    "    sampled_seq_list = []        # List of sampled sequnce from end to this step\n",
    "    sampled_seq_prob_list = []   # List of liklihood for th sampled sequnce from end to this step\n",
    "    sampled_seq_length_list = [] # List of lenght for sampled sequnce from end to this step\n",
    "    ## Split the search space for sequnce to differenr barnches\n",
    "    for beam in range(number_search_branch):\n",
    "        sampled_token_index = beam_top_token_indecies[beam]\n",
    "        sampled_token_prob  = output_tokens[0, -1, sampled_token_index]\n",
    "        if sampled_token_index == word2id['<EOS>'] or seq_length == decoder_seq_length:\n",
    "            return [sampled_token_index], sampled_token_prob, 0.00000001 # smalle number to avoid divde by zero\n",
    "        else:\n",
    "            ## Update the target sequence (of length 1).\n",
    "            decoder_input = np.zeros((1, 1, decoder_vocab_size))\n",
    "            decoder_input[0, 0, sampled_token_index] = 1\n",
    "            \n",
    "            ## recusrive call to decode_sequence_beam function itself to find\n",
    "            ## best sequnce from this point to the end\n",
    "            sampled_seq, sampled_seq_prob, sampled_seq_length = decode_sequence_beam(decoder_model, decoder_input, states_value, word2id, seq_length)\n",
    "            \n",
    "            ## Save the sampled sequnce (This a sampled sequnce from end to this point)\n",
    "            sampled_seq.append(sampled_token_index)\n",
    "\n",
    "            ## calculate the Sequnce probabity from end to this step\n",
    "            sampled_seq_prob *= sampled_token_prob\n",
    "            \n",
    "            ## Append the sampled sequnce to a list\n",
    "            sampled_seq_list.append(sampled_seq)\n",
    "            ## Append the sampled sequnce probability to a list\n",
    "            sampled_seq_prob_list.append(sampled_seq_prob)\n",
    "            ## Append the sampled sequnce length to a list\n",
    "            sampled_seq_length_list.append(sampled_seq_length)\n",
    "    \n",
    "    ## Claculate weighted probabity of list sequnces (bracnh beams) from end pof sequnce to this step\n",
    "    # The sequnce probabities are ajusted for lenght, thus model will not prefer shorter length\n",
    "    # sequnces. This is required because longer sequnces are generally have lower probability \n",
    "    weighted_prob = np.log(np.array(sampled_seq_prob_list))/np.array(sampled_seq_length_list)\n",
    "    \n",
    "    ## Choose a sequnce from beam branch with the highest probability \n",
    "    best_beam = np.argmax(weighted_prob)\n",
    "    \n",
    "    return sampled_seq_list[best_beam], sampled_seq_prob_list[best_beam], sampled_seq_length_list[best_beam]+1\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" The main steps to predict an output sequnce using the seq2seq model:\n",
    "    1. Read test dataset\n",
    "    2. Preproces each sequnce (create standarized sequnces)\n",
    "        a. Change QID and CONDITION text to lowercase\n",
    "        b. split QID and CONDITION text into tokens (words)\n",
    "        c. Replace QID tokens in each sample with standrized tokens (i.e., <QID0>, <QID1>, ...)\n",
    "        d. Replace digit tokens in each sample with standarized tokens (i.e., <DGT0>, <DGT1>, ...)\n",
    "        e. Create standardization dictionary for each sample\n",
    "        f. Add special tokens <BOS> and <EOS> to the begining and end of each sequence\n",
    "    3. Replace condition sequnce tokens with an integre id usng the encoder_word2id dictionary\n",
    "    4. Pad condition sequnce with zero to create a fixed size input sequnce\n",
    "        a. Input sequnce is pre-padded with zero\n",
    "    5. Extract Encoder and Decoder parts of saved seq2seq model\n",
    "    6. Use a beam search algorithm to predict the output sequnce\n",
    "    7. Reverse predicted output sequnce to words using the decoder_id2word dictionary \n",
    "    8. Revrese Digit and QID standardization from the predicted output\n",
    "    9. Save the precited outputs to a file\n",
    "    \"\"\"\n",
    "\n",
    "    # Test data path\n",
    "    test_data_path = MT_TEST_CORPUS_PATH\n",
    "\n",
    "    # Output data path\n",
    "    test_data_output_path = MT_TEST_CORPUS_PATH_WITH_PREDCITION\n",
    "\n",
    "    # Make sure an Encoder model exists\n",
    "    if not os.path.exists(MT_SEQ2SEQ_MODEL_PATH):\n",
    "        print(\"\\n The seq2seq model [%s] does not exist\\n\" % MT_SEQ2SEQ_MODEL_PATH)\n",
    "        return\n",
    "\n",
    "    # Load model and metadata\n",
    "    model = load_model(MT_SEQ2SEQ_MODEL_PATH)\n",
    "\n",
    "    with open(MT_META_DATA_FILE_PATH,'rb') as f:\n",
    "        [condition_word2id, condition_id2word, output_word2id, output_id2word] = pickle.load(f)\n",
    "\n",
    "    print(\"\\nLoaded a trained seq2seq model from [{}]\\n\".format(MT_SEQ2SEQ_MODEL_PATH))\n",
    "\n",
    "    encoder_model, decoder_model = create_seq2seq_inference_model(model, num_latent_dim)\n",
    "    \n",
    "    #test_data_path = MT_TRAINING_CORPUS_PATH\n",
    "    \n",
    "    #Read dataset from Excel file\n",
    "    qids_raw, conditions_raw, output_raw = read_data(test_data_path)\n",
    "    print(\"\\nLoaded test dataset from [{}]\\n\".format(test_data_path))\n",
    "\n",
    "    # Preprocess the raw input text data\n",
    "    _, conditions, _, dictionaries_lemanization = prepare_data(qids_raw, conditions_raw, output_raw)\n",
    "    \n",
    "    # Replace words of qid, condition and ouput with corresponding id in dictonaries\n",
    "    conditions = replace_using_dict(conditions, condition_word2id, drop_unknown=True)\n",
    "\n",
    "    # Fix all sequnces length to a fixed size with padding\n",
    "    conditions = pad_with_zero(conditions, encoder_seq_length,'pre')\n",
    "\n",
    "    outputs_predcited = [None for _ in conditions]\n",
    "    for sample_index, condition in enumerate(conditions):\n",
    "\n",
    "        input_seq = condition\n",
    "        decoded_seqeunce, _ = decode_sequence(input_seq, encoder_model, decoder_model, output_word2id, output_id2word)\n",
    "        \n",
    "        decoded_seqeunce = replace_using_dict([decoded_seqeunce], output_id2word)\n",
    "        decoded_seqeunce = replace_using_dict(decoded_seqeunce, dictionaries_lemanization[sample_index])\n",
    "\n",
    "        decoded_seqeunce = [seq for seq in decoded_seqeunce[0] if seq != '<PAD>' and seq != '<EOS>'\\\n",
    "                                                                and '<QID' not in seq and '<DGT' not in seq]\n",
    "        decoded_seqeunce = reversed(decoded_seqeunce)\n",
    "        decoded_seqeunce = ''.join(decoded_seqeunce)\n",
    "\n",
    "        outputs_predcited[sample_index] = decoded_seqeunce\n",
    "\n",
    "        if sample_index % 10 == 0:\n",
    "            log_to_shell(sample_index, qids_raw[sample_index],\n",
    "                           conditions_raw[sample_index], output_raw[sample_index],\n",
    "                           decoded_seqeunce )\n",
    "        \n",
    "    write_data(qids_raw, conditions_raw, outputs_predcited, test_data_output_path)\n",
    "    print(\"\\nSaved predictions to [{}]\\n\".format(test_data_output_path))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
